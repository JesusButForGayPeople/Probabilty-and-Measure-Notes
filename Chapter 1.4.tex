\UNsection{1.4 - Denumerable Probabilities}
\seteqgroup{4}

We will now further scrutinize the details concerning infinite sequences of events in a probability space. The following examples are given in the \textbf{Unit interval}, meaning that  $(\Omega, \mathcal{F}, P)$ is the triple where 
\[
\begin{gathered}
    \Omega = (0,1]\\
    \mathcal{F} = \mathfrak{B} = \sigma(\mathfrak{B}_0)\\
    P(A) = \lambda(A) \text{ for } A \in \mathcal{F}
\end{gathered}
\]

However, the definitions and theorems in the subsequent section apply to \textit{all} probability spaces.


\UNsubsection{General Formulas} \quad

If $P(A) > 0$, the \textbf{Conditional Probability} of $B$ given $A$ is defined familiarly:
\begin{UNequation}
    P(B|A)=\frac{P(A\cap B)}{P(A)}
\end{UNequation}

The following define the \textbf{Chain Rule Formulas}:
\begin{UNequation}
    \begin{gathered}
        P(A\cap B) = P(A)P(B|A),\\
        P(A\cap B \cap C) = P(A)P(B|A)P(C|A\cap P)\\
        \vdots
    \end{gathered}
\end{UNequation}

If $A_1,A_2,...$ partition $\Omega$, then

\vspace{-6.5ex}

\begin{UNequation}
   \hspace{2.5cm} P(B)= \sum_n P(A_n)P(B|A_n)
\end{UNequation}

For fixed $A$ the function $P(B|A)$ defines a probability measure as $B$ varies over $\mathcal{F}$.

\needspace{6\baselineskip}
\UNsubsection{Limit Sets} \quad

For a sequence \( A_1, A_2, \ldots \) of sets, define the sets
\vspace{-0.5ex}
\begin{UNequation}
\boldsymbol{\limsup}_{n} A_n = \bigcap_{n=1}^{\infty} \bigcup_{k=n}^{\infty} A_k \quad \scalebox{0.8}[0.8]{$\textbf{(Limit Superior of }\boldsymbol{A_n)}$}
\end{UNequation}
\vspace{-4.5ex}
\begin{UNequation}
\boldsymbol{\liminf}_{n} A_n = \bigcup_{n=1}^{\infty} \bigcap_{k=n}^{\infty} A_k. \quad \scalebox{0.8}[0.8]{$\textbf{(Limit Inferior of }\boldsymbol{A_n)}$}
\end{UNequation}

\textbf{Example:} Consider the functions \( d_n(\omega) \) defined on the unit interval by the dyadic expansion (1.1), and let \( \boldsymbol{\ell}(\omega) \) be the length of the run of 0's starting at \( d_n(\omega) \):  
\[
\boldsymbol{\ell}_n(\omega) = k \ \text{if both} \ 
\begin{array}{l}
    d_n(\omega) = \cdots = d_{n+k-1}(\omega) = 0\\
    \text{and} \quad d_{n+k}(\omega) = 1
\end{array} \ \Rightarrow\  \boldsymbol{\ell}_n(\omega) = 0 \text{ if } d_n(\omega) = 1
\]  
Probabilities can be computed using the following formula:
\begin{UNequation}
    P\left\{ \omega : d_i(\omega) = u_i,\ i = 1, \ldots, n \right\} = \frac{1}{2^n}
\end{UNequation} 

Since $\{ \omega : \boldsymbol{\ell}_n(\omega) = k \}$ is a union of \( 2^{n-1} \) disjoint intervals of length \( 2^{-n-k} \), it lies in \( \mathcal{F} \) and has probability \( 2^{-k-1} \). Therefore,  \(\displaystyle \{ \omega : \boldsymbol{\ell}_n(\omega) \geq r \} = \{ \omega : d_i(\omega) = 0,\, n \leq i < n + r \}\)  
lies also in \( \mathcal{F} \) and has probability $\sum_{k \geq r} 2^{-k-1}$.

\begin{equation}
P\left( \omega : \boldsymbol{\ell}_n(\omega) \geq r \right) = 2^{-r}.
\tag{4.7}
\end{equation}

If \( A_n \) is the event in (4.7), then (4.4) is the set of \( \omega \) such that \( l_n(\omega) \geq r \) for infinitely many \( n \), or, \( n \) being regarded as a time index, such that \( \boldsymbol{\ell}_n(\omega) \geq r \) \textit{infinitely often}.

\UNsubsection{Independent Events} \quad

Events $A$ and $B$ are \textbf{Independent} if $P(A\cap B)=P(A)P(B)$. For events with positive probability, this is the same thing as requiring $P(B|A)=P(B)$ or $P(A|B)=P(A)$. More generally, a finite collection $A_1,...,A_n$ of events is independent if:
\begin{UNequation}
    P(A_{k_1}\cap\cdots \cap A_{k_j})=P(A_{k_j})\cdots P(A_{k_j})
\end{UNequation}

\vspace{-3ex}
for $2\leq j \leq n$ and $1 \leq k_1 < \cdots < k_j \leq n$. Reordering the sets clearly has no effect on the condition for independence, and a subcollection of independent events is also independent. An infinite (perhaps uncountable) collection of events is defined to be independent in each of its finite subcollections is.
\begin{UNequation}
        \text{If }n=3 \begin{cases}
        \text{for } j=2 \text{ the three constraints are:}\\
        \phantom{for } P(A_1\cap A_2)=P(A_1)P(A_2)\\
        \phantom{for }P(A_1\cap A_3)=P(A_1)P(A_3)\\
        \phantom{for }P(A_2\cap A_3)=P(A_2)P(A_3)\\
        \text{and if } j=3 \text{ the single constraint is:}\\
        \phantom{for }P(A_1\cap A_2 \cap A_3)= P(A_1)P(A_2)P(A_3)
        \end{cases}
\end{UNequation}

\vspace{5pt}

\textbf{Theorem 4.2: } If $\mathcal{A}_1, ..., \mathcal{A}_n$ are independent and each $\mathcal{A}_i$ is a $\pi$-system, them, $\sigma(\mathcal{A}_1),...,\sigma\mathcal{A}_n$ are independent.\\

\textbf{Proof:}
\vspace{-1ex}
\begin{proofline}
	Let $\mathcal{B}_i$ be the class $\mathcal{A}_i$ augmented by $\Omega$. Then for each $\mathfrak{B}_i$ is a $\pi$-system, then the independence hypothesis holds if $B_i \in \mathfrak{B}_i, \ i=1,...,n$. For fixed sets $B_2,...,B_n$ lying respectively in $\mathfrak{B}_2,...,\mathfrak{B}_n$, let $\mathcal{L}$ be the class of $\mathcal{F}$-sets $B_1$ for which the hypothesis of independence holds. Then $\mathcal{L}$ is a $\lambda$-system containing the $\pi$-system $\mathfrak{B}_1$ and hence containing $\sigma(\mathfrak{B}_1)=\sigma(\mathcal{A}_1$. Therefore, they are independent if $B_1, B_2, ..., B_n$ lie respectively in $\sigma(\mathcal{A}_1), \mathfrak{B}_2,...,\mathfrak{B}_n$, which means that $\sigma(\mathcal{A}_1), \mathcal{A}_2,...,\mathcal{A}_n$ are independent. Clearly, the argument holds for any other choice of index that is less than or equal to $n$. \hfill \qed
\end{proofline}

\phantom{aaa}\elbowarrow \textbf{Corollary 1: } If $\mathcal{A}_{\theta}$, $\theta \in \Theta$ are independent and each $\mathcal{A}_{\theta}$ is a $\pi$-system,\\
\phantom{aaa\elbowarrow}then $\sigma(\mathcal{A}_{\theta})$, $\theta \in \Theta$, are independent.

\vspace{1ex}

\phantom{aaa}\elbowarrow  \textbf{Corollary 2: } Suppose that the array $\Rightarrow$

\vspace{-6ex}

\begin{UNequation}
    \hspace{6cm}\begin{array}{cccc}
        A_{11} & A_{12} & \cdots \\
        A_{21} & A_{22} & \cdots \\
        \vdots & \vdots & \ddots \\
    \end{array}
\end{UNequation}

\vspace{-11ex}

\phantom{aaa\elbowarrow}of events is independent; here each row\\
\phantom{aaa\elbowarrow}is a finite or infinite sequence, and there\\
\phantom{aaa\elbowarrow}are finitely or infinitely many rows.\\[1ex]
\phantom{aaa\elbowarrow}If $\mathcal{F}_i$ is the $\sigma$-field generated by the $i$th row, then $\mathcal{F}_1, \mathcal{F}_2, \dots$ are independent.

\UNsubsection{Subfields} \quad

The definition of independence provided above involves several different $\sigma$-fields at once, which is not all that uncommon to see within probability theory. However, measure theory for its own sake typically involves a single all-encompassing $\sigma$-field $\mathcal{F}$. A subclass $\mathcal{A}$ of $\mathcal{F}$ corresponds to \textbf{partial information}.

\vspace{2ex}

Imagine that a point $\omega$ is drawn from $\Omega$ according to the probabilities given by $P$ (``$\omega$ lies in $A$ with probability $P(A)$'') Imagine also that an observer who does not know which $\omega$ has been drawn but who does know for each $A \in \mathcal{A}$ whether $\omega \in A$ or $\omega \notin A$ (meaning that they do not know $\omega$ but they do know the values of $I_A{\omega}$ for each $A\in \mathcal{A}$). Identifying this partial information with the class $\mathcal{A}$? itself will illuminate the connection between various measure-theoretic concepts and the premathematical ideas lying behind them.

\vspace{2ex}

The set $B$ is by definition independent of the class $\mathcal{A}$ if $P(A|B)=P(B)$ for all sets $A \in \mathcal{A}$ for which $P(A)>0$. Thus if $B$ is independent of $\mathcal{A}$, then  the observer's probability for $B$ is $P(B)$ even after he has received the information in $\mathcal{A}$; in this case $\mathcal{A}$ contains no information about $B$. This remains true even if the observer has all the information contained in $\sigma(\mathcal{A})$.

\vspace{2ex}

The notion of partial information can be looked at in terms of partitions. We say that points $\omega$ and $\omega'$ are $\boldsymbol{\mathcal{A}}$\textbf{-equivalent}, if for every $A$ in $\mathcal{A}$, $\omega$ and $\omega'$ lie either both in $A$ or both in $A^c$, that is if:

\vspace{-4.5ex}

\begin{UNequation}
    \hspace{3cm} I_A(\omega)=I_A(\omega'), \quad \forall A \in \mathcal{A}
\end{UNequation}

\vspace{-4.5ex}

This relation partitions $\Omega$ into sets of equivalent points; call this the $\boldsymbol{\mathcal{A}}$\textbf{-partition}.

\vspace{2ex}

\textbf{Example:} If $\omega$ and $\omega'$ are $\sigma(\mathcal{A})$-equivalent, then certainly they are $\mathcal{A}$-equivalent. For fixed $\omega$ and $\omega'$, the class of $A$ such that $I_A(\omega) = I_A(\omega')$ is a $\sigma$-field; if $\omega$ and $\omega'$ are $\mathcal{A}$-equivalent, then this $\sigma$-field contains $\mathcal{A}$ and hence $\sigma(\mathcal{A})$, so that $\omega$ and $\omega'$ are also $\sigma(\mathcal{A})$-equivalent. Thus $\mathcal{A}$-equivalence and $\sigma(\mathcal{A})$-equivalence are the same thing, and the $\mathcal{A}$-partition coincides with the $\sigma(\mathcal{A})$-partition. \hfill \qed

\vspace{2ex}

An observer with the information in $\sigma(\mathcal{A})$ knows, not the point $\omega$ drawn, but only the equivalence class containing it. That is exactly the information they have. One who knows that $\omega$ lies in a set $A$ has more information about $\omega$ the smaller 
$A$ is. One who knows $I_A(\omega)$ for each $A$ in class $\mathcal{A}$, however has more information about $\omega$ the larger $\mathcal{A}$ is. Furthermore, to have the information that $\mathcal{A}_1$ and the information in $\mathcal{A}_2$ is to have the information in $\mathcal{A}_1 \cup \mathcal{A}_2$, not that in $\mathcal{A}_1 \cap \mathcal{A}_2$.

\vspace{2ex}

\textbf{Example:} In the unit interval $(\Omega, \mathcal{F}, P)$ let $\mathcal{G}$ be the $\sigma$-field consisting of the countable and the co-countable sets. Since $P(G)$ is $0$ or $1$ for each $G$ in $\mathcal{G}$, each set $H$ in $\mathcal{F}$ is independent of $\mathcal{G}$. But in this case the $\mathcal{G}$-partition consists of the singletons, and so the information in $\mathcal{G}$ tells the observer exactly which $\omega$ in $\Omega$ has been drawn.
\begin{enumerate}[label=\textbf{\roman*.}, topsep=1pt, itemsep=0.5ex]
    \item The $\sigma$-field $\mathcal{G}$ contains \textit{no} information about $H$—in the sense that $H$ and $\mathcal{G}$ are independent. \item The $\sigma$-field $\mathcal{G}$ contains \textit{all} the information about $H$—in the sense that it tells the observer exactly which $\omega$ was drawn. \hfill \qed
\end{enumerate}

\vspace{2ex}

The two conclusions of the above example appear to stand in direct contradiction of one another. However, the first conclusion (\textbf{i}) is mathematically rigorous -- $H$ and $\mathcal{G}$ are independent -- whereas the second conclusion (\textbf{ii}) only concerns a particular interpretation. This is a result of the unusual structure of the $\sigma$-field $\mathcal{G}$ rather than a shortcoming in the definition of independence. The heuristic of equating $\sigma$-fields and information is helpful, even if sometimes it breaks down. It is important to remember that the definition of independence takes precedence when the heuristic interpretation appears to contradict it.


\UNsubsection{The Borel-Cantelli Lemmas} \quad

\vspace{2ex}

\textbf{The First Borel-Cantelli Lemma}

\textbf{Theorem 4.3: } if $\sum_n P(A_n)$ converges, then $P(\boldsymbol{\limsup}_n\ A_n)=0$

\textbf{Proof:}
\vspace{-1ex}
\begin{proofline}
    $$\limsup_n A_n \subset \bigcup_{k=m}^{\infty} A_k \ =\text{implies}\Rightarrow\ P(\limsup_n A_n) \leq P\left(\bigcup_{k=m}^{\infty} A_k\right) \leq \sum_{k=m}^{\infty} P(A_k)$$ and this sum tends to $0$ as $m \to \infty$ if $\sum_n P(A_n)$ converges. \hfill \qed
\end{proofline}

\vspace{2ex}

\textbf{The Second Borel-Cantelli Lemma}

\textbf{Theorem 4.4: } if $\{A_n\}$ is an independent sequence of events and $\sum_n P(A_n)$ diverges, then $P(\boldsymbol{\limsup}_n\ A_n)=1$

\textbf{Proof:}
\vspace{-1ex}
\begin{proofline}
    It is enough to prove that $P\left( \bigcup_{n=1}^\infty \bigcap_{k=n}^\infty A_k^c \right) = 0$ and hence enough to prove that $P\left( \bigcap_{k=n}^\infty A_k^c \right) = 0$ for all $n$. Since $1 - x \leq e^{-x}$,
    \[ 
    P\left( \bigcap_{k=n}^{n+j} A_k^c \right) = \prod_{k=n}^{n+j} (1 - P(A_k)) \leq \exp\left[ -\sum_{k=n}^{n+j} P(A_k) \right].
    \]
    Since $\sum_k P(A_k)$ diverges, the last expression tends to $0$ as $j \to \infty$, and hence $P\left( \bigcap_{k=n}^\infty A_k^c \right) = \lim_j P\left( \bigcap_{k=n}^{n+j} A_k^c \right) = 0$. \hfill \qed
\end{proofline}

\vspace{2ex}

\UNsubsection{The Zero-One Law} \quad
For a sequence $A_1, A_2, ...$ of events in a probability space $(\Omega, \mathcal{F}, P)$ consider the $\sigma$-fields $\sigma(A_n,A_{n+1},...)$ annd their intersection
\begin{UNequation}
    \mathcal{T} = \bigcap_{n=1}^{\infty}\sigma(A_n,A_{n+1},...)
\end{UNequation}

This is called the \textbf{tail} $\boldsymbol{\sigma}$\textbf{-field} associated with the sequence $\{A_n\}$, and its elements are called \textbf{tail events}. The idea is that a tail event is determined solely by the $A_n$ for arbitrarily large $n.$

\vspace{2ex}

\needspace{6\baselineskip}
\textbf{Theorem 4.5 (Kolmogorov's zero-one law): } If $A_1, A_2, \dots$ is an independent sequence of events, then for each event $A$ in the tail $\sigma$-field (4.10), $P(A)$ is either $0$ or $1$.

\textbf{Proof:}
\vspace{-1ex}
\begin{proofline}
    By Corollary 2 to Theorem 4.2, $\sigma(A_1), \dots, \sigma(A_{n-1}), \sigma(A_n, A_{n+1}, \dots)$ are independent. If $A \in \mathcal{T}$, then $A \in \sigma(A_n, A_{n+1}, \dots)$ and therefore $A_1, \dots, A_{n-1}, A$ are independent. Since independence of a collection of events is defined by independence of each finite subcollection, the sequence $A, A_1, A_2, \dots$ is independent. By a second application of Corollary 2 to Theorem 4.2, $\sigma(A)$ and $\sigma(A_1, A_2, \dots)$ are independent. But $A \in \mathcal{T} \subset \sigma(A_1, A_2, \dots)$; from $A \in \sigma(A)$ and $A \in \sigma(A_1, A_2, \dots)$ it follows that $A$ is independent of itself: $P(A \cap A) = P(A)P(A)$. This is the same as $P(A) = (P(A))^2$ and can hold only if $P(A)$ is $0$ or $1$. \hfill \qed
\end{proofline}