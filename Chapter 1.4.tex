\UNsection{1.4 - Denumerable Probabilities}
\seteqgroup{4}

We will now further scrutinize the details concerning infinite sequences of events in a probability space. The following examples are given in the \textbf{Unit interval}, meaning that  $(\Omega, \mathcal{F}, P)$ is the triple where 
\[
\begin{gathered}
    \Omega = (0,1]\\
    \mathcal{F} = \mathfrak{B} = \sigma(\mathfrak{B}_0)\\
    P(A) = \lambda(A) \text{ for } A \in \mathcal{F}
\end{gathered}
\]

However, the definitions and theorems in the subsequent section apply to \textit{all} probability spaces.


\UNsubsection{General Formulas} \quad

If $P(A) > 0$, the \textbf{Conditional Probability} of $B$ given $A$ is defined familiarly:
\begin{UNequation}
    P(B|A)=\frac{P(A\cap B)}{P(A)}
\end{UNequation}

The following define the \textbf{Chain Rule Formulas}:
\begin{UNequation}
    \begin{gathered}
        P(A\cap B) = P(A)P(B|A),\\
        P(A\cap B \cap C) = P(A)P(B|A)P(C|A\cap P)\\
        \vdots
    \end{gathered}
\end{UNequation}

If $A_1,A_2,...$ partition $\Omega$, then

\vspace{-6.5ex}

\begin{UNequation}
   \hspace{2.5cm} P(B)= \sum_n P(A_n)P(B|A_n)
\end{UNequation}

For fixed $A$ the function $P(B|A)$ defines a probability measure as $B$ varies over $\mathcal{F}$.

\needspace{6\baselineskip}
\UNsubsection{Limit Sets} \quad

For a sequence \( A_1, A_2, \ldots \) of sets, define the sets
\vspace{-0.5ex}
\begin{UNequation}
\boldsymbol{\limsup}_{n} A_n = \bigcap_{n=1}^{\infty} \bigcup_{k=n}^{\infty} A_k \quad \scalebox{0.8}[0.8]{$\textbf{(Limit Superior of }\boldsymbol{A_n)}$}
\end{UNequation}
\vspace{-4.5ex}
\begin{UNequation}
\boldsymbol{\liminf}_{n} A_n = \bigcup_{n=1}^{\infty} \bigcap_{k=n}^{\infty} A_k. \quad \scalebox{0.8}[0.8]{$\textbf{(Limit Inferior of }\boldsymbol{A_n)}$}
\end{UNequation}

\textbf{Example:} Consider the functions \( d_n(\omega) \) defined on the unit interval by the dyadic expansion (1.1), and let \( \boldsymbol{\ell}(\omega) \) be the length of the run of 0's starting at \( d_n(\omega) \):  
\[
\boldsymbol{\ell}_n(\omega) = k \ \text{if both} \ 
\begin{array}{l}
    d_n(\omega) = \cdots = d_{n+k-1}(\omega) = 0\\
    \text{and} \quad d_{n+k}(\omega) = 1
\end{array} \ \Rightarrow\  \boldsymbol{\ell}_n(\omega) = 0 \text{ if } d_n(\omega) = 1
\]  
Probabilities can be computed using the following formula:
\begin{UNequation}
    P\left\{ \omega : d_i(\omega) = u_i,\ i = 1, \ldots, n \right\} = \frac{1}{2^n}
\end{UNequation} 

Since $\{ \omega : \boldsymbol{\ell}_n(\omega) = k \}$ is a union of \( 2^{n-1} \) disjoint intervals of length \( 2^{-n-k} \), it lies in \( \mathcal{F} \) and has probability \( 2^{-k-1} \). Therefore,  \(\displaystyle \{ \omega : \boldsymbol{\ell}_n(\omega) \geq r \} = \{ \omega : d_i(\omega) = 0,\, n \leq i < n + r \}\)  
lies also in \( \mathcal{F} \) and has probability $\sum_{k \geq r} 2^{-k-1}$.

\begin{equation}
P\left( \omega : \boldsymbol{\ell}_n(\omega) \geq r \right) = 2^{-r}.
\tag{4.7}
\end{equation}

If \( A_n \) is the event in (4.7), then (4.4) is the set of \( \omega \) such that \( l_n(\omega) \geq r \) for infinitely many \( n \), or, \( n \) being regarded as a time index, such that \( \boldsymbol{\ell}_n(\omega) \geq r \) \textit{infinitely often}.

\UNsubsection{Independent Events} \quad

Events $A$ and $B$ are \textbf{Independent} if $P(A\cap B)=P(A)P(B)$. For events with positive probability, this is the same thing as requiring $P(B|A)=P(B)$ or $P(A|B)=P(A)$. More generally, a finite collection $A_1,...,A_n$ of events is independent if:
\begin{UNequation}
    P(A_{k_1}\cap\cdots \cap A_{k_j})=P(A_{k_j})\cdots P(A_{k_j})
\end{UNequation}

\vspace{-3ex}
for $2\leq j \leq n$ and $1 \leq k_1 < \cdots < k_j \leq n$. Reordering the sets clearly has no effect on the condition for independence, and a subcollection of independent events is also independent. An infinite (perhaps uncountable) collection of events is defined to be independent in each of its finite subcollections is.
\begin{UNequation}
        \text{If }n=3 \begin{cases}
        \text{for } j=2 \text{ the three constraints are:}\\
        \phantom{for } P(A_1\cap A_2)=P(A_1)P(A_2)\\
        \phantom{for }P(A_1\cap A_3)=P(A_1)P(A_3)\\
        \phantom{for }P(A_2\cap A_3)=P(A_2)P(A_3)\\
        \text{and if } j=3 \text{ the single constraint is:}\\
        \phantom{for }P(A_1\cap A_2 \cap A_3)= P(A_1)P(A_2)P(A_3)
        \end{cases}
\end{UNequation}

\vspace{5pt}

\textbf{Theorem 4.2: } If $\mathcal{A}_1, ..., \mathcal{A}_n$ are independent and each $\mathcal{A}_i$ is a $\pi$-system, them, $\sigma(\mathcal{A}_1),...,\sigma\mathcal{A}_n$ are independent.\\

\textbf{Proof:}
\vspace{-1ex}
\begin{proofline}
	Let $\mathcal{B}_i$ be the class $\mathcal{A}_i$ augmented by $\Omega$. Then for each $\mathfrak{B}_i$ is a $\pi$-system, then the independence hypothesis holds if $B_i \in \mathfrak{B}_i, \ i=1,...,n$. For fixed sets $B_2,...,B_n$ lying respectively in $\mathfrak{B}_2,...,\mathfrak{B}_n$, let $\mathcal{L}$ be the class of $\mathcal{F}$-sets $B_1$ for which the hypothesis of independence holds. Then $\mathcal{L}$ is a $\lambda$-system containing the $\pi$-system $\mathfrak{B}_1$ and hence containing $\sigma(\mathfrak{B}_1)=\sigma(\mathcal{A}_1$. Therefore, they are independent if $B_1, B_2, ..., B_n$ lie respectively in $\sigma(\mathcal{A}_1), \mathfrak{B}_2,...,\mathfrak{B}_n$, which means that $\sigma(\mathcal{A}_1), \mathcal{A}_2,...,\mathcal{A}_n$ are independent. Clearly, the argument holds for any other choice of index that is less than or equal to $n$. \hfill \qed
\end{proofline}

\elbowarrow \textbf{Corollary 1: } If $\mathcal{A}_{\theta}$, $\theta \in \Theta$ are independent and each $\mathcal{A}_{\theta}$ is a $\pi$-system, then $\sigma(\mathcal{A}_{\theta})$, $\theta \in \Theta$, are independent.

\UNsubsection{Subfields} \quad

\newpage